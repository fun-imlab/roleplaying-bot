import streamlit as st
from langchain_community.chat_models import ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores.faiss import FAISS as LCFAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
import base64
import os
import math



# --- Google Sheetsèªè¨¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆ1å›ã ã‘ã§OKï¼‰ ---
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive"
]
creds = Credentials.from_service_account_file("credentials.json", scopes=SCOPES)
gc = gspread.authorize(creds)

# --- æ›¸ãè¾¼ã¿å…ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’æŒ‡å®šï¼ˆURLã®/d/ã¨/editã®é–“ã®éƒ¨åˆ†ï¼‰---
SPREADSHEET_ID = "1C3roVQgqCNQCjEsZCcI7zY1UXEp5fboQDAa9ERDufFY"
sh = gc.open_by_key(SPREADSHEET_ID)
worksheet = sh.sheet1  # 1æšç›®ã®ã‚·ãƒ¼ãƒˆã‚’ä½¿ã†


# --- Streamlit UI ---
st.set_page_config(page_title="äººå·¥çŸ¥èƒ½åŸºç¤ è§’å…ˆç”ŸBot", layout="wide")
st.title("ğŸ“ äººå·¥çŸ¥èƒ½åŸºç¤ è§’å…ˆç”ŸBot")

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
os.environ["OPENAI_API_KEY"] = openai_api_key

# --- ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆsumi.jpegï¼‰ã‚’Base64å½¢å¼ã§èª­ã¿è¾¼ã¿ ---
def load_image_as_base64(image_path):
    with open(image_path, "rb") as f:
        data = f.read()
    return base64.b64encode(data).decode()

img_base64 = load_image_as_base64("sumi.jpeg")  # â† å…ˆç”Ÿç”¨ã®ç”»åƒã‚’æŒ‡å®š

# --- ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§embeddingã‚’ãƒãƒƒãƒé€ä¿¡ã—ã€embeddingæ¸ˆã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’FAISSã«ç™»éŒ² ---
def batch_faiss_from_documents(docs, embedding, batch_size=400):
    texts = [doc.page_content for doc in docs]
    metadatas = [doc.metadata for doc in docs]
    all_embeddings = []
    total = len(texts)
    batches = math.ceil(total / batch_size)
    for i in range(batches):
        start = i * batch_size
        end = min(start + batch_size, total)
        batch = texts[start:end]
        batch_embeds = embedding.embed_documents(batch)
        all_embeddings.extend(batch_embeds)

    # FAISSã¸ã®ç™»éŒ²ï¼ˆæœ€æ–°ç‰ˆã«å¯¾å¿œï¼‰
    text_embeddings = list(zip(texts, all_embeddings))
    vectordb = LCFAISS.from_embeddings(
        text_embeddings=text_embeddings,
        embedding=embedding,
        metadatas=metadatas
    )
    return vectordb

# --- RAGç”¨ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰ ---
@st.cache_resource
def load_vectorstore():
    loader = TextLoader("rag_trainning.txt", encoding="utf-8")
    documents = loader.load()
    splitter = CharacterTextSplitter(
        separator="ã€‚",
        chunk_size=500,
        chunk_overlap=50
    )
    docs = splitter.split_documents(documents)
    embedding = OpenAIEmbeddings()
    vectordb = batch_faiss_from_documents(docs, embedding, batch_size=400)
    return vectordb

vectordb = load_vectorstore()
retriever = vectordb.as_retriever(search_kwargs={"k": 3})

template = """
ã‚ãªãŸã¯Aå…ˆç”Ÿæœ¬äººã¨ã—ã¦ã€è¬›ç¾©ã«å‚åŠ ã—ãŸå­¦ç”Ÿã‹ã‚‰ã®è³ªå•ã«ç­”ãˆã¾ã™ã€‚
å£èª¿ãƒ»èªå°¾ãƒ»è©±ã—æ–¹ã®ç™–ãƒ»æ€è€ƒã®ç‰¹å¾´ãªã©ã¯ã€ä»¥ä¸‹ã®è¬›ç¾©ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¿ å®Ÿã«å­¦ã³ã€å†ç¾ã—ã¦ãã ã•ã„ã€‚
ç¬¬ä½•å›ã®ãªã‚“ã¨ã„ã†ã‚¿ã‚¤ãƒˆãƒ«ã®æˆæ¥­ã§ã®è©±é¡Œã‹ã‚‚åˆã‚ã›ã¦ç­”ãˆã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã¯Aå…ˆç”ŸãŒå®Ÿéš›ã«è¬›ç¾©ä¸­ã«è©±ã—ãŸå†…å®¹ã§ã™ï¼š

=========
{context}
=========

è³ªå•ï¼š
{question}

Aå…ˆç”Ÿã¨ã—ã¦ã€ã¾ã‚‹ã§â€œä»Šã“ã®å ´ã§ã‚ãªãŸãŒå­¦ç”Ÿã«èªã£ã¦ã„ã‚‹ã‹ã®ã‚ˆã†ã«â€å›ç­”ã—ã¦ãã ã•ã„ã€‚
æ–‡ç« ã¯è‡ªç„¶ãªè©±ã—è¨€è‘‰ã§ã€å¥èª­ç‚¹ã‚„èªå°¾ãªã©ã‚‚å®Ÿéš›ã®å£èª¿ã«è¿‘ã¥ã‘ã¦ãã ã•ã„ã€‚
"""
prompt_template = PromptTemplate.from_template(template)

llm = ChatOpenAI(model_name="gpt-4.1") 
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt_template},
    return_source_documents=True
)

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿æŒ ---
if "history" not in st.session_state:
    st.session_state.history = []

# --- å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ  ---
with st.form(key="chat_form", clear_on_submit=True):
    query = st.text_input("ğŸ’¬ è¬›ç¾©ã«åŸºã¥ã„ã¦è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„")
    submitted = st.form_submit_button("é€ä¿¡")

if submitted and query:
    with st.spinner("è€ƒãˆä¸­..."):
        result = qa(query)
        st.session_state.history.append({
            "query": query,
            "answer": result["result"],
            "sources": [doc.page_content for doc in result["source_documents"]]
        })

        # ç¾åœ¨æ™‚åˆ»ã‚’æ—¥æœ¬æ™‚é–“ã§å–å¾—ï¼ˆä¾‹: 2024-07-24 20:25:33ï¼‰
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        # Google Sheetsã«ã€Œæ™‚åˆ»ã€ã¨ã€Œè³ªå•æ–‡ã€ã ã‘ã‚’è¿½åŠ 
        worksheet.append_row([now_str, query])

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’æ–°ã—ã„é †ã«ä¸Šã‹ã‚‰è¡¨ç¤ºã™ã‚‹ ---
for idx, chat in reversed(list(enumerate(st.session_state.history))):
    with st.container():
        st.markdown(f"#### ğŸ§‘â€ğŸ“ è³ªå• {idx+1}")
        st.write(chat["query"])

        # âœ…ã€å…ˆç”Ÿã®ã‚¢ã‚¤ã‚³ãƒ³ç”»åƒã‚’æŒ‡å®šã®ç”»åƒã«å¤‰æ›´ã€‘
        st.markdown(
            """
            <div style='display: flex; align-items: center;'>
                <img src='data:image/jpeg;base64,{}' width='50' height='50' style='border-radius:50%;'>
                <strong style='font-size:20px; margin-left:10px;'>è§’å…ˆç”Ÿã®å›ç­”ï¼š</strong>
            </div>
            """.format(img_base64),
            unsafe_allow_html=True
        )

        st.success(chat["answer"])

        # å‚è€ƒæ–‡æ›¸ã‚’expanderã§
        with st.expander("â–¶ï¸ å‚è€ƒã«ä½¿ã‚ã‚ŒãŸè¬›ç¾©ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦‹ã‚‹"):
            for s_idx, src in enumerate(chat["sources"]):
                st.markdown(f"**{s_idx+1} :** {src}")

        st.markdown("---")
