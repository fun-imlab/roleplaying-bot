import streamlit as st
from langchain_community.chat_models import ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import os

# --- Streamlit UI ---
st.set_page_config(page_title="è§’å…ˆç”Ÿãªã‚Šãã‚ŠChatBot", layout="wide")
st.title("ğŸ“ è§’å…ˆç”Ÿãªã‚Šãã‚ŠChatBot")


load_dotenv()  # .env ã‚’èª­ã¿è¾¼ã‚€
openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

os.environ["OPENAI_API_KEY"] = openai_api_key


# --- RAGç”¨ãƒ™ã‚¯ãƒˆãƒ«DBã®æ§‹ç¯‰ ---
@st.cache_resource
def load_vectorstore():
    loader = TextLoader("rag_trainning.txt", encoding="utf-8")
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.split_documents(documents)

    embedding = OpenAIEmbeddings()
    
    # âœ… ä¿®æ­£ç®‡æ‰€ï¼špersist_directory ã‚’å‰Šé™¤ï¼ˆï¼ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªå‹•ä½œã«ï¼‰
    vectordb = FAISS.from_documents(docs, embedding=embedding)
        
    return vectordb


vectordb = load_vectorstore()
retriever = vectordb.as_retriever()


template = """
ã‚ãªãŸã¯Aå…ˆç”Ÿæœ¬äººã¨ã—ã¦ã€è¬›ç¾©ã«å‚åŠ ã—ãŸå­¦ç”Ÿã‹ã‚‰ã®è³ªå•ã«ç­”ãˆã¾ã™ã€‚
å£èª¿ãƒ»èªå°¾ãƒ»è©±ã—æ–¹ã®ç™–ãƒ»æ€è€ƒã®ç‰¹å¾´ãªã©ã¯ã€ä»¥ä¸‹ã®è¬›ç¾©ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¿ å®Ÿã«å­¦ã³ã€å†ç¾ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã¯Aå…ˆç”ŸãŒå®Ÿéš›ã«è¬›ç¾©ä¸­ã«è©±ã—ãŸå†…å®¹ã§ã™ï¼š

=========
{context}
=========

è³ªå•ï¼š
{question}

Aå…ˆç”Ÿã¨ã—ã¦ã€ã¾ã‚‹ã§â€œä»Šã“ã®å ´ã§ã‚ãªãŸãŒå­¦ç”Ÿã«èªã£ã¦ã„ã‚‹ã‹ã®ã‚ˆã†ã«â€å›ç­”ã—ã¦ãã ã•ã„ã€‚
æ–‡ç« ã¯è‡ªç„¶ãªè©±ã—è¨€è‘‰ã§ã€å¥èª­ç‚¹ã‚„èªå°¾ãªã©ã‚‚å®Ÿéš›ã®å£èª¿ã«è¿‘ã¥ã‘ã¦ãã ã•ã„ã€‚
"""

prompt_template = PromptTemplate.from_template(template)

# --- LLM + æ¤œç´¢ãƒã‚§ãƒ¼ãƒ³ ---
llm = ChatOpenAI(model_name="gpt-4")
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt_template},
    return_source_documents=True
)

# --- ãƒãƒ£ãƒƒãƒˆå…¥åŠ›UI ---
query = st.text_input("ğŸ’¬ è¬›ç¾©ã«åŸºã¥ã„ã¦è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„")
if query:
    with st.spinner("è€ƒãˆä¸­..."):
        result = qa(query)
        st.success("âœ… å›ç­”")
        st.write(result["result"])

        # å‚è€ƒæ–‡æ›¸ã®è¡¨ç¤º
        st.markdown("### ğŸ” å‚è€ƒã«ä½¿ã‚ã‚ŒãŸè¬›ç¾©ãƒ†ã‚­ã‚¹ãƒˆ")
        for doc in result["source_documents"]:
            st.write(doc.page_content)
